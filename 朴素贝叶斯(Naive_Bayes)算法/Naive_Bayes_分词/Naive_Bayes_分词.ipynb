{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数功能： 创建实验数据集\n",
    "参数说明： 无\n",
    "返回：\n",
    "    dataSet：切分好的样本词条\n",
    "    classVec：类标签向量\n",
    "'''\n",
    "def loadDataSet():\n",
    "    dataSet =[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "              ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "              ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "              ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "              ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "              ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
    "             ] #切分好的词条\n",
    "    classVec = [0,1,0,1,0,1] # 类别标签向量，1代表侮辱性词汇，0代表非侮辱性词汇\n",
    "    return dataSet , classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet , classVec = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数功能：将切分的样本词条整理成词汇表\n",
    "参数说明：\n",
    "    dataSet：切分好的样本词条\n",
    "返回：\n",
    "    vocabList：不重复的词汇表\n",
    "'''\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set() # 创建一个空的set()-->去重\n",
    "    for doc in dataSet: # 遍历dataSet中的每一条言论\n",
    "        vocabSet = vocabSet | set(doc) #两个set集合取并集\n",
    "        vocaList = list(vocabSet)\n",
    "    return vocaList # 不重复的词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maybe', 'ate', 'not', 'flea', 'is', 'has', 'quit', 'my', 'food', 'dog', 'I', 'stupid', 'park', 'how', 'buying', 'please', 'stop', 'cute', 'so', 'problems', 'dalmation', 'licks', 'love', 'help', 'to', 'garbage', 'posting', 'mr', 'take', 'steak', 'worthless', 'him']\n"
     ]
    }
   ],
   "source": [
    "vocabList = createVocabList(dataSet)\n",
    "print(vocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数功能：根据vocabList词汇表，将inputSet（待测样本）向量化，向量的每一个元素为1 or 0\n",
    "参数说明：\n",
    "    vocabList: 词汇表\n",
    "    inputSet：切分好的词条列表中的一条（待测样本）\n",
    "返回：\n",
    "    returnVec：文档向量，词集模型\n",
    "'''\n",
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    returnVec = [0] * len(vocabList) # 创建一个其中所含元素都为0的向量\n",
    "    for word in inputSet: # 遍历每个词条\n",
    "        if word in inputSet: # 如果词条存在于词汇表中，则变为1\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(f\"{word} is not in my Vocabulary!\")\n",
    "    return returnVec #　返回文档向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数功能：生成训练集向量列表\n",
    "参数说明：\n",
    "    dataSet: 切分好的样本词条\n",
    "返回：\n",
    "    trainMat: 所有词条向量组成的列表\n",
    "'''\n",
    "def get_trainMat(dataSet):\n",
    "    trainMat = [] # 初始化向量列表\n",
    "    vocabList = createVocabList(dataSet) #生成词汇表\n",
    "    for inputSet in dataSet: # 遍历样本词条中的每一条样本\n",
    "        returnVec = setOfWords2Vec(vocabList,inputSet) # 将当前词条向量化\n",
    "        trainMat.append(returnVec) # 追加到向量列表中\n",
    "    return trainMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "trainMat = get_trainMat(dataSet)\n",
    "print(trainMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数功能: 朴素贝叶斯分类器训练函数\n",
    "参数说明：\n",
    "    trainMat: 训练文档矩阵\n",
    "    classVec：训练类别标签向量\n",
    "返回:\n",
    "    p0v: 非侮辱类的条件概率数组\n",
    "    p1v: 侮辱类的条件概率数组\n",
    "    pAb: 文档属于侮辱类的概率\n",
    "    \n",
    "初始化问题：\n",
    "\n",
    "拉普拉斯平滑（见pdf文档）\n",
    "\n",
    "利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，\n",
    "即计算 p(w0|1)p(w1|1)p(w2|1)。如果其中有一个概率值为0，那么最后的成绩也为0。\n",
    "显然，这样是不合理的，为了降低 这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2。\n",
    "这种做法就叫做拉普拉斯平滑(Laplace Smoothing)又被称为加1平滑，是比较常用的平滑方法，它就是为了解决0概率问题。\n",
    " \n",
    "'''\n",
    "def trainNB(trainMat,classVec):\n",
    "    n = len(trainMat) # 计算训练文档的数目\n",
    "    #print(n) ---> 6 \n",
    "    m = len(trainMat[0]) # 计算每篇文档的词条数\n",
    "    # print(m)---->32-->所有的不重复的单词共有32个\n",
    "    pAb = sum(classVec)/n # 文档属于侮辱类的概率\n",
    "    # print(pAb)--->0.5 ---> 3/6\n",
    "    p0Num = np.ones(m) # 词条出现初始化为1-->列表长度为32\n",
    "    p1Num = np.ones(m) # 词条出现初始化为1\n",
    "    p0Denom = 1 # 分母初始化为1\n",
    "    p1Denom = 1 # 分母初始化为1\n",
    "    for i in range(n):  # 遍历每一个文档\n",
    "        if classVec[i] == 1: # 统计属于侮辱类的条件概率所需的数据\n",
    "            p1Num += trainMat[i]\n",
    "            p1Denom += sum(trainMat[i])\n",
    "        else: # 统计属于非侮辱类的条件概率所需的数据\n",
    "            p0Num += trainMat[i]\n",
    "            p0Denom += sum(trainMat[i])\n",
    "    p1v = np.log(p1Num / p1Denom)\n",
    "    p0v = np.log(p0Num / p0Denom)\n",
    "    return p0v , p1v , pAb  #返回属于非侮辱类，侮辱类和文档属于侮辱类的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n",
      "**************************************************\n",
      "[0, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(trainMat)\n",
    "print('*'* 50)\n",
    "print(classVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0v,p1v,pAb = trainNB(trainMat,classVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.21887582, -2.52572864, -3.21887582, -2.52572864, -2.52572864,\n",
       "       -2.52572864, -3.21887582, -1.83258146, -3.21887582, -2.52572864,\n",
       "       -2.52572864, -3.21887582, -3.21887582, -2.52572864, -3.21887582,\n",
       "       -2.52572864, -2.52572864, -2.52572864, -2.52572864, -2.52572864,\n",
       "       -2.52572864, -2.52572864, -2.52572864, -2.52572864, -2.52572864,\n",
       "       -3.21887582, -3.21887582, -2.52572864, -3.21887582, -2.52572864,\n",
       "       -3.21887582, -2.12026354])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.30258509, -2.99573227, -2.30258509, -2.99573227, -2.99573227,\n",
       "       -2.99573227, -2.30258509, -2.99573227, -2.30258509, -1.89711998,\n",
       "       -2.99573227, -1.60943791, -2.30258509, -2.99573227, -2.30258509,\n",
       "       -2.99573227, -2.30258509, -2.99573227, -2.99573227, -2.99573227,\n",
       "       -2.99573227, -2.99573227, -2.99573227, -2.99573227, -2.30258509,\n",
       "       -2.30258509, -2.30258509, -2.99573227, -2.30258509, -2.99573227,\n",
       "       -1.89711998, -2.30258509])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数功能：朴素贝叶斯分类器分类函数\n",
    "参数说明：\n",
    "    vec2classify: 待分类的词条数组\n",
    "    p0V: 非侮辱类的条件概率数组\n",
    "    p1V：侮辱类的条件概率数组\n",
    "    pAb：文档属于侮辱类的概率\n",
    "返回：\n",
    "    0：属于非侮辱类\n",
    "    1：属于侮辱类\n",
    "'''\n",
    "def classifyNB(vec2classify,p0V,p1V,pAb):\n",
    "    p1 = sum(vec2classify * p1V) +  np.log(pAb) # 对应元素相乘-->根据log函数的特点（loga + logb = log a*b）\n",
    "    p0 = sum(vec2classify * p0V) +  np.log(1-pAb)\n",
    "    print('p0:',p0)\n",
    "    print('p1:',p1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数功能：朴素贝叶斯测试函数\n",
    "参数说明：\n",
    "    testVec：待测样本\n",
    "返回：\n",
    "    测试样本类别\n",
    "'''\n",
    "def testingNB(testVec):\n",
    "    dataSet,classVec = loadDataSet() # 创建实验样本\n",
    "    vocabList = createVocabList(dataSet) # 创建词汇表\n",
    "    trainMat = get_trainMat(dataSet) # 将实验样本向量化\n",
    "    p0V , p1V ,pAb = trainNB(trainMat,classVec) # 训练朴素贝叶斯分类器\n",
    "    thisone = setOfWords2Vec(vocabList,testVec) # 测试样本向量化\n",
    "    if classifyNB(thisone,p0V,p1V,pAb): # 执行分类\n",
    "        print(testVec,\"属于侮辱类\")\n",
    "    else:\n",
    "        print(testVec,\"属于非侮辱类\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0: -7.577185932924767\n",
      "p1: -9.680344001221918\n",
      "['love', 'my', 'dalmation'] 属于非侮辱类\n",
      "p0: -7.1308988302963465\n",
      "p1: -4.605170185988091\n",
      "['stupid', 'garbage'] 属于侮辱类\n"
     ]
    }
   ],
   "source": [
    "#测试样本1 \n",
    "testVec1 = ['love', 'my', 'dalmation'] \n",
    "testingNB(testVec1) \n",
    "#测试样本2 \n",
    "testVec2 = ['stupid', 'garbage'] \n",
    "testingNB(testVec2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
